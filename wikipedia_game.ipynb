{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Wikipedia Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set a wikipedia url as input.\n",
    "* Extract all links from the page.\n",
    "* Go to a random other page.\n",
    "* Check if one of these links follows back to the inital page.\n",
    "* If yes, you succeeded.\n",
    "* If no follow randomly links, until you reach the initial page.\n",
    "* Print the number of needed pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle wikipedia links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us try to extract all links from the Wikipedia page of \"Python (programming language)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1654\n"
     ]
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Python_(programming_language)\"\n",
    "\n",
    "html = urlopen(url)   \n",
    "soup = bs(html.read())\n",
    "all_links = []\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "    current_link = link.get('href')\n",
    "    all_links.append(current_link)\n",
    "\n",
    "print(len(all_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply some string manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = [link for link in all_links if link is not None] # remove all none types\n",
    "all_links = all_links[3:] # delete standard Wikipedia head\n",
    "\n",
    "bad_string = [\"$\", \"%\", \"&\", \":\", \"Main_Page\", \"ISO\"] # exclude extensions with this characters\n",
    "temp = []\n",
    "for i in range(0, len(all_links)):\n",
    "    if any(string in all_links[i] for string in bad_string):\n",
    "        continue\n",
    "    else:\n",
    "        temp.append(all_links[i])\n",
    "all_links = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to only keep these links that yield to other Wikipedia pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/wiki/Python_(disambiguation)', '/wiki/Programming_paradigm', '/wiki/Multi-paradigm_programming_language', '/wiki/Functional_programming', '/wiki/Imperative_programming', '/wiki/Object-oriented_programming', '/wiki/Reflective_programming', '/wiki/Software_design', '/wiki/Guido_van_Rossum']\n",
      "738\n"
     ]
    }
   ],
   "source": [
    "substring = \"/wiki/\"\n",
    "\n",
    "wiki_links = []\n",
    "for i in range(0, len(all_links)):\n",
    "    if all_links[i].startswith(substring):\n",
    "        wiki_links.append(all_links[i])\n",
    "        \n",
    "print(wiki_links[0:9])\n",
    "print(len(wiki_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extracted the endings of the Wikipedia URLs. Now we try to reformulate parsable URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://en.wikipedia.org/wiki/Python_(disambiguation)', 'https://en.wikipedia.org/wiki/Programming_paradigm', 'https://en.wikipedia.org/wiki/Multi-paradigm_programming_language', 'https://en.wikipedia.org/wiki/Functional_programming', 'https://en.wikipedia.org/wiki/Imperative_programming', 'https://en.wikipedia.org/wiki/Object-oriented_programming', 'https://en.wikipedia.org/wiki/Reflective_programming', 'https://en.wikipedia.org/wiki/Software_design', 'https://en.wikipedia.org/wiki/Guido_van_Rossum']\n"
     ]
    }
   ],
   "source": [
    "substring = \"https://en.wikipedia.org\"\n",
    "\n",
    "wiki_urls = []\n",
    "\n",
    "for i in range(0, len(wiki_links)):\n",
    "    url = substring + wiki_links[i]\n",
    "    wiki_urls.append(url)\n",
    "    \n",
    "print(wiki_urls[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_winner(input_url):\n",
    "    \n",
    "    # read html\n",
    "    html = urlopen(input_url)   \n",
    "    soup = bs(html.read())\n",
    "    \n",
    "    # extract links\n",
    "    all_links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        current_link = link.get('href')\n",
    "        all_links.append(current_link)\n",
    "    \n",
    "    all_links = [link for link in all_links if link is not None]\n",
    "    all_links = all_links[3:]\n",
    "    \n",
    "    bad_string = [\"$\", \"%\", \"&\", \":\", \"Main_Page\", \"ISO\"] # exclude extensions with this characters\n",
    "    temp = []\n",
    "    for i in range(0, len(all_links)):\n",
    "        if any(string in all_links[i] for string in bad_string):\n",
    "            continue\n",
    "        else:\n",
    "            temp.append(all_links[i])\n",
    "    all_links = temp\n",
    "    \n",
    "    # keep only wiki links\n",
    "    substring = \"/wiki/\"\n",
    "    wiki_links = []\n",
    "    for i in range(0, len(all_links)):\n",
    "        if all_links[i].startswith(substring):\n",
    "            wiki_links.append(all_links[i])\n",
    "    \n",
    "    # generate wiki urls\n",
    "    substring = \"https://en.wikipedia.org\"\n",
    "    wiki_urls = []\n",
    "    for i in range(0, len(wiki_links)):\n",
    "        url = substring + wiki_links[i]\n",
    "        wiki_urls.append(url)\n",
    "        \n",
    "    # delete input_url and choose random url\n",
    "    if input_url in wiki_urls: wiki_urls.remove(input_url)\n",
    "    current_url = rd.choice(wiki_urls)\n",
    "    \n",
    "    # initialize counter\n",
    "    counter = 1\n",
    "    \n",
    "    # parse and scrape pages until initial page is reached\n",
    "    while current_url != input_url:\n",
    "        \n",
    "        print(\"Iteration \", counter, \": \", current_url)\n",
    "        \n",
    "        # again read html\n",
    "        html = urlopen(current_url)\n",
    "        soup = bs(html.read())\n",
    "        \n",
    "        # again extract links\n",
    "        all_links = []\n",
    "        for link in soup.find_all('a'):\n",
    "            current_link = link.get('href')\n",
    "            all_links.append(current_link)\n",
    "        \n",
    "        all_links = [link for link in all_links if link is not None]\n",
    "        all_links = all_links[3:]\n",
    "        \n",
    "        bad_string = [\"$\", \"%\", \"&\", \":\", \"Main_Page\", \"ISO\"] # exclude extensions with this characters\n",
    "        temp = []\n",
    "        for i in range(0, len(all_links)):\n",
    "            if any(string in all_links[i] for string in bad_string):\n",
    "                continue\n",
    "            else:\n",
    "                temp.append(all_links[i])\n",
    "        all_links = temp\n",
    "        \n",
    "        # again keep only wiki links\n",
    "        substring = \"/wiki/\"\n",
    "        wiki_links = []\n",
    "        for i in range(0, len(all_links)):\n",
    "            if all_links[i].startswith(substring):\n",
    "                wiki_links.append(all_links[i])\n",
    "        \n",
    "        # again generate wiki urls\n",
    "        substring = \"https://en.wikipedia.org\"\n",
    "        wiki_urls = []\n",
    "        for i in range(0, len(wiki_links)):\n",
    "            url = substring + wiki_links[i]\n",
    "            wiki_urls.append(url)\n",
    "        \n",
    "        # search for input url\n",
    "        if input_url in wiki_urls:\n",
    "            print(\"-----------------------------\")\n",
    "            print(\"You succeeded in \", counter, \" steps\")\n",
    "            print(\"You reached your input URL: \", input_url)\n",
    "            print(\"-----------------------------\")\n",
    "            break\n",
    "        else:\n",
    "            current_url = rd.choice(wiki_urls)\n",
    "            counter += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  1 :  https://en.wikipedia.org/wiki/Printing\n",
      "Iteration  2 :  https://en.wikipedia.org/wiki/Goryeo\n",
      "Iteration  3 :  https://en.wikipedia.org/wiki/Chongchon_River\n",
      "Iteration  4 :  https://en.wikipedia.org/wiki/Lake_Rangrim\n",
      "Iteration  5 :  https://en.wikipedia.org/wiki/Daedong_Bay_Important_Bird_Area\n",
      "Iteration  6 :  https://en.wikipedia.org/wiki/Black-faced_spoonbill\n",
      "Iteration  7 :  https://en.wikipedia.org/wiki/Green_ibis\n",
      "Iteration  8 :  https://en.wikipedia.org/wiki/Paraguay\n",
      "Iteration  9 :  https://en.wikipedia.org/wiki/Central_Bank_of_Paraguay\n",
      "Iteration  10 :  https://en.wikipedia.org/wiki/Discount_window\n",
      "Iteration  11 :  https://en.wikipedia.org/wiki/Early_2000s_recession\n",
      "-----------------------------\n",
      "You succeeded in  11  steps\n",
      "You reached your input URL:  https://en.wikipedia.org/wiki/Germany\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Germany\"\n",
    "wiki_winner(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  1 :  https://en.wikipedia.org/wiki/George_Orwell\n",
      "Iteration  2 :  https://en.wikipedia.org/wiki/The_Road_to_Wigan_Pier\n",
      "Iteration  3 :  https://en.wikipedia.org/wiki/Radio_Four\n",
      "Iteration  4 :  https://en.wikipedia.org/wiki/Isle_of_Man\n",
      "Iteration  5 :  https://en.wikipedia.org/wiki/Culture_of_Ireland\n",
      "Iteration  6 :  https://en.wikipedia.org/wiki/Trinity\n",
      "Iteration  7 :  https://en.wikipedia.org/wiki/Dead_Sea_Scrolls\n",
      "Iteration  8 :  https://en.wikipedia.org/wiki/Aramaic_Enoch_Scroll\n",
      "Iteration  9 :  https://en.wikipedia.org/wiki/7Q5\n",
      "Iteration  10 :  https://en.wikipedia.org/wiki/Temple_Scroll\n",
      "Iteration  11 :  https://en.wikipedia.org/wiki/Torah\n",
      "Iteration  12 :  https://en.wikipedia.org/wiki/Summa_Theologica\n",
      "Iteration  13 :  https://en.wikipedia.org/wiki/Thomism\n",
      "Iteration  14 :  https://en.wikipedia.org/wiki/Edmund_Husserl\n",
      "Iteration  15 :  https://en.wikipedia.org/wiki/Authenticity_(philosophy)\n",
      "Iteration  16 :  https://en.wikipedia.org/wiki/Walter_Kaufmann_(philosopher)\n",
      "Iteration  17 :  https://en.wikipedia.org/wiki/WorldCat_Identities\n",
      "Iteration  18 :  https://en.wikipedia.org/wiki/Margalit_Fox\n",
      "Iteration  19 :  https://en.wikipedia.org/wiki/Seamus_Heaney\n",
      "Iteration  20 :  https://en.wikipedia.org/wiki/Harold_Pinter\n",
      "Iteration  21 :  https://en.wikipedia.org/wiki/Karl_Adolph_Gjellerup\n",
      "Iteration  22 :  https://en.wikipedia.org/wiki/Erik_Axel_Karlfeldt\n",
      "Iteration  23 :  https://en.wikipedia.org/wiki/Elias_Canetti\n",
      "Iteration  24 :  https://en.wikipedia.org/wiki/Rainald_Goetz\n",
      "Iteration  25 :  https://en.wikipedia.org/wiki/German_language\n",
      "Iteration  26 :  https://en.wikipedia.org/wiki/Politics_of_Germany\n",
      "Iteration  27 :  https://en.wikipedia.org/wiki/Politics_of_Ukraine\n",
      "Iteration  28 :  https://en.wikipedia.org/wiki/President_of_Ukraine\n",
      "Iteration  29 :  https://en.wikipedia.org/wiki/Constitution_of_Ukraine\n",
      "Iteration  30 :  https://en.wikipedia.org/wiki/Valeriy_Lobanovskyi_Dynamo_Stadium\n",
      "Iteration  31 :  https://en.wikipedia.org/wiki/Arena_Lviv\n",
      "Iteration  32 :  https://en.wikipedia.org/wiki/UEFA_Euro_2012_Group_B#Germany_vs_Portugal\n",
      "Iteration  33 :  https://en.wikipedia.org/wiki/Nani\n",
      "Iteration  34 :  https://en.wikipedia.org/wiki/Portugal\n",
      "Iteration  35 :  https://en.wikipedia.org/wiki/Information_technology\n",
      "Iteration  36 :  https://en.wikipedia.org/wiki/PubMed_Identifier\n",
      "Iteration  37 :  https://en.wikipedia.org/wiki/Patent\n",
      "Iteration  38 :  https://en.wikipedia.org/wiki/Abandonware\n",
      "Iteration  39 :  https://en.wikipedia.org/wiki/Internet_Archive\n",
      "-----------------------------\n",
      "You succeeded in  39  steps\n",
      "You reached your input URL:  https://en.wikipedia.org/wiki/Facebook\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Facebook\"\n",
    "wiki_winner(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
