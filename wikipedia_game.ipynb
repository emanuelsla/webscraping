{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Wikipedia Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set a wikipedia url as input.\n",
    "* Extract all links from the page.\n",
    "* Go to a random other page.\n",
    "* Check if one of these links follows back to the inital page.\n",
    "* If yes, you succeeded.\n",
    "* If no follow randomly links, until you reach the initial page.\n",
    "* Print the number of needed pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle wikipedia links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us try to extract all links from the Wikipedia page of \"Python (programming language)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1654\n"
     ]
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Python_(programming_language)\"\n",
    "\n",
    "html = urlopen(url)   \n",
    "soup = bs(html.read())\n",
    "all_links = []\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "    current_link = link.get('href')\n",
    "    all_links.append(current_link)\n",
    "\n",
    "print(len(all_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply some string manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = [link for link in all_links if link is not None] # remove all none types\n",
    "all_links = all_links[3:] # delete standard Wikipedia head\n",
    "\n",
    "bad_string = [\"$\", \"%\", \"&\", \":\", \"Main_Page\", \"ISO\"] # exclude extensions with this characters\n",
    "temp = []\n",
    "for i in range(0, len(all_links)):\n",
    "    if any(string in all_links[i] for string in bad_string):\n",
    "        continue\n",
    "    else:\n",
    "        temp.append(all_links[i])\n",
    "all_links = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to only keep these links that yield to other Wikipedia pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/wiki/Python_(disambiguation)', '/wiki/Programming_paradigm', '/wiki/Multi-paradigm_programming_language', '/wiki/Functional_programming', '/wiki/Imperative_programming', '/wiki/Object-oriented_programming', '/wiki/Reflective_programming', '/wiki/Software_design', '/wiki/Guido_van_Rossum']\n",
      "738\n"
     ]
    }
   ],
   "source": [
    "substring = \"/wiki/\"\n",
    "\n",
    "wiki_links = []\n",
    "for i in range(0, len(all_links)):\n",
    "    if all_links[i].startswith(substring):\n",
    "        wiki_links.append(all_links[i])\n",
    "        \n",
    "print(wiki_links[0:9])\n",
    "print(len(wiki_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extracted the endings of the Wikipedia URLs. Now we try to reformulate parsable URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://en.wikipedia.org/wiki/Python_(disambiguation)', 'https://en.wikipedia.org/wiki/Programming_paradigm', 'https://en.wikipedia.org/wiki/Multi-paradigm_programming_language', 'https://en.wikipedia.org/wiki/Functional_programming', 'https://en.wikipedia.org/wiki/Imperative_programming', 'https://en.wikipedia.org/wiki/Object-oriented_programming', 'https://en.wikipedia.org/wiki/Reflective_programming', 'https://en.wikipedia.org/wiki/Software_design', 'https://en.wikipedia.org/wiki/Guido_van_Rossum']\n"
     ]
    }
   ],
   "source": [
    "substring = \"https://en.wikipedia.org\"\n",
    "\n",
    "wiki_urls = []\n",
    "\n",
    "for i in range(0, len(wiki_links)):\n",
    "    url = substring + wiki_links[i]\n",
    "    wiki_urls.append(url)\n",
    "    \n",
    "print(wiki_urls[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_winner(input_url):\n",
    "    \n",
    "    # read html\n",
    "    html = urlopen(input_url)   \n",
    "    soup = bs(html.read())\n",
    "    \n",
    "    # extract links\n",
    "    all_links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        current_link = link.get('href')\n",
    "        all_links.append(current_link)\n",
    "    \n",
    "    all_links = [link for link in all_links if link is not None]\n",
    "    all_links = all_links[3:]\n",
    "    \n",
    "    bad_string = [\"$\", \"%\", \"&\", \":\", \"Main_Page\", \"ISO\"] # exclude extensions with this characters\n",
    "    temp = []\n",
    "    for i in range(0, len(all_links)):\n",
    "        if any(string in all_links[i] for string in bad_string):\n",
    "            continue\n",
    "        else:\n",
    "            temp.append(all_links[i])\n",
    "    all_links = temp\n",
    "    \n",
    "    # keep only wiki links\n",
    "    substring = \"/wiki/\"\n",
    "    wiki_links = []\n",
    "    for i in range(0, len(all_links)):\n",
    "        if all_links[i].startswith(substring):\n",
    "            wiki_links.append(all_links[i])\n",
    "    \n",
    "    # generate wiki urls\n",
    "    substring = \"https://en.wikipedia.org\"\n",
    "    wiki_urls = []\n",
    "    for i in range(0, len(wiki_links)):\n",
    "        url = substring + wiki_links[i]\n",
    "        wiki_urls.append(url)\n",
    "        \n",
    "    # delete input_url and choose random url\n",
    "    if input_url in wiki_urls: wiki_urls.remove(input_url)\n",
    "    current_url = rd.choice(wiki_urls)\n",
    "    \n",
    "    # initialize counter\n",
    "    counter = 1\n",
    "    \n",
    "    # parse and scrape pages until initial page is reached\n",
    "    while current_url != input_url:\n",
    "        \n",
    "        print(\"Iteration \", counter, \": \", current_url)\n",
    "        \n",
    "        # again read html\n",
    "        html = urlopen(current_url)\n",
    "        soup = bs(html.read())\n",
    "        \n",
    "        # again extract links\n",
    "        all_links = []\n",
    "        for link in soup.find_all('a'):\n",
    "            current_link = link.get('href')\n",
    "            all_links.append(current_link)\n",
    "        \n",
    "        all_links = [link for link in all_links if link is not None]\n",
    "        all_links = all_links[3:]\n",
    "        \n",
    "        bad_string = [\"$\", \"%\", \"&\", \":\", \"Main_Page\", \"ISO\"] # exclude extensions with this characters\n",
    "        temp = []\n",
    "        for i in range(0, len(all_links)):\n",
    "            if any(string in all_links[i] for string in bad_string):\n",
    "                continue\n",
    "            else:\n",
    "                temp.append(all_links[i])\n",
    "        all_links = temp\n",
    "        \n",
    "        # again keep only wiki links\n",
    "        substring = \"/wiki/\"\n",
    "        wiki_links = []\n",
    "        for i in range(0, len(all_links)):\n",
    "            if all_links[i].startswith(substring):\n",
    "                wiki_links.append(all_links[i])\n",
    "        \n",
    "        # again generate wiki urls\n",
    "        substring = \"https://en.wikipedia.org\"\n",
    "        wiki_urls = []\n",
    "        for i in range(0, len(wiki_links)):\n",
    "            url = substring + wiki_links[i]\n",
    "            wiki_urls.append(url)\n",
    "        \n",
    "        # search for input url\n",
    "        if input_url in wiki_urls:\n",
    "            print(\"-----------------------------\")\n",
    "            print(\"You succeeded in \", counter, \" steps\")\n",
    "            print(\"You reached your input URL: \", input_url)\n",
    "            print(\"-----------------------------\")\n",
    "            break\n",
    "        else:\n",
    "            current_url = rd.choice(wiki_urls)\n",
    "            counter += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  1 :  https://en.wikipedia.org/wiki/Albert_Einstein\n",
      "Iteration  2 :  https://en.wikipedia.org/wiki/Violin\n",
      "Iteration  3 :  https://en.wikipedia.org/wiki/International_Standard_Book_Number\n",
      "Iteration  4 :  https://en.wikipedia.org/wiki/ISO_216\n",
      "Iteration  5 :  https://en.wikipedia.org/wiki/MPEG-4_Part_11\n",
      "Iteration  6 :  https://en.wikipedia.org/wiki/MPEG-4\n",
      "Iteration  7 :  https://en.wikipedia.org/wiki/FDI_World_Dental_Federation_notation\n",
      "Iteration  8 :  https://en.wikipedia.org/wiki/Z_notation\n",
      "Iteration  9 :  https://en.wikipedia.org/wiki/ISO_15189\n",
      "Iteration  10 :  https://en.wikipedia.org/wiki/ISO_26262\n",
      "Iteration  11 :  https://en.wikipedia.org/wiki/Risk_Management\n",
      "Iteration  12 :  https://en.wikipedia.org/wiki/Professional_Diving_Instructors_Corporation\n",
      "Iteration  13 :  https://en.wikipedia.org/wiki/Felix_Hoppe-Seyler\n",
      "-----------------------------\n",
      "You succeeded in  13  steps\n",
      "You reached your input URL:  https://en.wikipedia.org/wiki/Germany\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Germany\"\n",
    "wiki_winner(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  1 :  https://en.wikipedia.org/wiki/Public_key_certificate\n",
      "Iteration  2 :  https://en.wikipedia.org/wiki/Convergence_(SSL)\n",
      "Iteration  3 :  https://en.wikipedia.org/wiki/Goto_fail\n",
      "Iteration  4 :  https://en.wikipedia.org/wiki/Common_subexpression_elimination\n",
      "Iteration  5 :  https://en.wikipedia.org/wiki/Instruction_scheduling\n",
      "Iteration  6 :  https://en.wikipedia.org/wiki/Loop_optimization\n",
      "Iteration  7 :  https://en.wikipedia.org/wiki/Induction_variable\n",
      "Iteration  8 :  https://en.wikipedia.org/wiki/Jump_threading\n",
      "Iteration  9 :  https://en.wikipedia.org/wiki/Software_pipelining\n",
      "Iteration  10 :  https://en.wikipedia.org/wiki/Assembly_code\n",
      "Iteration  11 :  https://en.wikipedia.org/wiki/X86\n",
      "Iteration  12 :  https://en.wikipedia.org/wiki/Endianness\n",
      "Iteration  13 :  https://en.wikipedia.org/wiki/File_Allocation_Table\n",
      "Iteration  14 :  https://en.wikipedia.org/wiki/High_Performance_File_System\n",
      "Iteration  15 :  https://en.wikipedia.org/wiki/JFS_(file_system)\n",
      "Iteration  16 :  https://en.wikipedia.org/wiki/Configfs\n",
      "Iteration  17 :  https://en.wikipedia.org/wiki/Universal_Disk_Format\n",
      "Iteration  18 :  https://en.wikipedia.org/wiki/Record-oriented_filesystem\n",
      "Iteration  19 :  https://en.wikipedia.org/wiki/ZFS\n",
      "Iteration  20 :  https://en.wikipedia.org/wiki/Amiga_Old_File_System\n",
      "Iteration  21 :  https://en.wikipedia.org/wiki/XAD_(software)\n",
      "Iteration  22 :  https://en.wikipedia.org/wiki/File_archiver\n",
      "Iteration  23 :  https://en.wikipedia.org/wiki/WinRAR\n",
      "Iteration  24 :  https://en.wikipedia.org/wiki/Freeware\n",
      "Iteration  25 :  https://en.wikipedia.org/wiki/Independent_soft_drink\n",
      "Iteration  26 :  https://en.wikipedia.org/wiki/Cassette_culture\n",
      "Iteration  27 :  https://en.wikipedia.org/wiki/Homebrewing\n",
      "Iteration  28 :  https://en.wikipedia.org/wiki/Webtoon\n",
      "Iteration  29 :  https://en.wikipedia.org/wiki/Chuseok\n",
      "Iteration  30 :  https://en.wikipedia.org/wiki/Bonfire\n",
      "Iteration  31 :  https://en.wikipedia.org/wiki/Effigy\n",
      "Iteration  32 :  https://en.wikipedia.org/wiki/Guy_Fawkes_Night\n",
      "Iteration  33 :  https://en.wikipedia.org/wiki/Restoration_(England)\n",
      "Iteration  34 :  https://en.wikipedia.org/wiki/House_of_Tudor\n",
      "Iteration  35 :  https://en.wikipedia.org/wiki/Romanian_royal_family\n",
      "Iteration  36 :  https://en.wikipedia.org/wiki/List_of_Bulgarian_monarchs\n",
      "Iteration  37 :  https://en.wikipedia.org/wiki/List_of_mammals_of_Bulgaria\n",
      "Iteration  38 :  https://en.wikipedia.org/wiki/Marbled_polecat\n",
      "Iteration  39 :  https://en.wikipedia.org/wiki/Leopard_seal\n",
      "Iteration  40 :  https://en.wikipedia.org/wiki/African_palm_civet\n",
      "Iteration  41 :  https://en.wikipedia.org/wiki/Ocelot\n",
      "Iteration  42 :  https://en.wikipedia.org/wiki/Hooded_skunk\n",
      "Iteration  43 :  https://en.wikipedia.org/wiki/Sulawesi_palm_civet\n",
      "Iteration  44 :  https://en.wikipedia.org/wiki/Malagasy_civet\n",
      "Iteration  45 :  https://en.wikipedia.org/wiki/Reginald_Innes_Pocock\n",
      "Iteration  46 :  https://en.wikipedia.org/wiki/Entomology\n",
      "Iteration  47 :  https://en.wikipedia.org/wiki/Moth\n",
      "Iteration  48 :  https://en.wikipedia.org/wiki/Maggot\n",
      "Iteration  49 :  https://en.wikipedia.org/wiki/Polychaete\n",
      "Iteration  50 :  https://en.wikipedia.org/wiki/Diurodrilidae\n",
      "Iteration  51 :  https://en.wikipedia.org/wiki/PubMed_Identifier\n",
      "Iteration  52 :  https://en.wikipedia.org/wiki/Health_information_on_the_Internet\n",
      "Iteration  53 :  https://en.wikipedia.org/wiki/Digital_object_identifier\n",
      "Iteration  54 :  https://en.wikipedia.org/wiki/Guidelines_for_the_Definition_of_Managed_Objects\n",
      "Iteration  55 :  https://en.wikipedia.org/wiki/IEC_60958\n",
      "Iteration  56 :  https://en.wikipedia.org/wiki/IEC_62196\n",
      "Iteration  57 :  https://en.wikipedia.org/wiki/IEC_61030\n",
      "Iteration  58 :  https://en.wikipedia.org/wiki/VXS\n",
      "Iteration  59 :  https://en.wikipedia.org/wiki/Advanced_Host_Controller_Interface\n",
      "Iteration  60 :  https://en.wikipedia.org/wiki/Motherboard\n",
      "Iteration  61 :  https://en.wikipedia.org/wiki/Computer_speakers\n",
      "Iteration  62 :  https://en.wikipedia.org/wiki/Touchscreen\n",
      "Iteration  63 :  https://en.wikipedia.org/wiki/Insulator_(electrical)\n",
      "Iteration  64 :  https://en.wikipedia.org/wiki/Strain_insulator\n",
      "Iteration  65 :  https://en.wikipedia.org/wiki/Strain_insulator\n",
      "Iteration  66 :  https://en.wikipedia.org/wiki/Collectable\n",
      "Iteration  67 :  https://en.wikipedia.org/wiki/Special_edition\n",
      "Iteration  68 :  https://en.wikipedia.org/wiki/Souvenir\n",
      "Iteration  69 :  https://en.wikipedia.org/wiki/BEST_Education_Network\n",
      "Iteration  70 :  https://en.wikipedia.org/wiki/Journey_planner\n",
      "Iteration  71 :  https://en.wikipedia.org/wiki/Geographic_data_and_information\n",
      "Iteration  72 :  https://en.wikipedia.org/wiki/Geomatics\n",
      "Iteration  73 :  https://en.wikipedia.org/wiki/Photogrammetry\n",
      "Iteration  74 :  https://en.wikipedia.org/wiki/Digital_object_identifier\n",
      "Iteration  75 :  https://en.wikipedia.org/wiki/CAE_number\n",
      "Iteration  76 :  https://en.wikipedia.org/wiki/CAE_number\n",
      "Iteration  77 :  https://en.wikipedia.org/wiki/International_Standard_Link_Identifier\n",
      "Iteration  78 :  https://en.wikipedia.org/wiki/Service_Modeling_Language\n",
      "Iteration  79 :  https://en.wikipedia.org/wiki/Re-use\n",
      "Iteration  80 :  https://en.wikipedia.org/wiki/Sewage_regulation_and_administration\n",
      "Iteration  81 :  https://en.wikipedia.org/wiki/Demolition_waste\n",
      "Iteration  82 :  https://en.wikipedia.org/wiki/Incineration\n",
      "Iteration  83 :  https://en.wikipedia.org/wiki/EfW\n",
      "Iteration  84 :  https://en.wikipedia.org/wiki/Asphalt_concrete#Recycling\n",
      "Iteration  85 :  https://en.wikipedia.org/wiki/Intersection_(road)\n",
      "Iteration  86 :  https://en.wikipedia.org/wiki/Atlantic_Media_Company\n",
      "Iteration  87 :  https://en.wikipedia.org/wiki/The_Hotline\n",
      "Iteration  88 :  https://en.wikipedia.org/wiki/National_Journal\n",
      "Iteration  89 :  https://en.wikipedia.org/wiki/HuffPost\n",
      "Iteration  90 :  https://en.wikipedia.org/wiki/Robert_Reich\n",
      "Iteration  91 :  https://en.wikipedia.org/wiki/Martin_Neil_Baily\n",
      "Iteration  92 :  https://en.wikipedia.org/wiki/Gardner_Ackley\n",
      "Iteration  93 :  https://en.wikipedia.org/wiki/Diplomat\n",
      "Iteration  94 :  https://en.wikipedia.org/wiki/Citizen\n",
      "Iteration  95 :  https://en.wikipedia.org/wiki/England\n",
      "Iteration  96 :  https://en.wikipedia.org/wiki/Montserrat\n",
      "Iteration  97 :  https://en.wikipedia.org/wiki/United_Kingdom\n",
      "Iteration  98 :  https://en.wikipedia.org/wiki/Cricket_World_Cup\n",
      "Iteration  99 :  https://en.wikipedia.org/wiki/ISTAF_World_Cup\n",
      "Iteration  100 :  https://en.wikipedia.org/wiki/2011_ISTAF_World_Cup\n",
      "Iteration  101 :  https://en.wikipedia.org/wiki/Switzerland\n",
      "Iteration  102 :  https://en.wikipedia.org/wiki/Alps\n",
      "Iteration  103 :  https://en.wikipedia.org/wiki/Ortler\n",
      "Iteration  104 :  https://en.wikipedia.org/wiki/List_of_mountain_lists\n",
      "Iteration  105 :  https://en.wikipedia.org/wiki/Four-thousand_footers\n",
      "Iteration  106 :  https://en.wikipedia.org/wiki/Crocker_Mountain_(Maine)\n",
      "Iteration  107 :  https://en.wikipedia.org/wiki/Fairfield,_Maine\n",
      "Iteration  108 :  https://en.wikipedia.org/wiki/New_Vineyard,_Maine\n",
      "Iteration  109 :  https://en.wikipedia.org/wiki/Narrows_Pond\n",
      "Iteration  110 :  https://en.wikipedia.org/wiki/Drainage_basin\n",
      "Iteration  111 :  https://en.wikipedia.org/wiki/Helicoidal_flow\n",
      "Iteration  112 :  https://en.wikipedia.org/wiki/Corrasion\n",
      "Iteration  113 :  https://en.wikipedia.org/wiki/Hydraulic_action\n",
      "Iteration  114 :  https://en.wikipedia.org/wiki/OCLC\n",
      "Iteration  115 :  https://en.wikipedia.org/wiki/Frederick_G._Kilgour\n",
      "Iteration  116 :  https://en.wikipedia.org/wiki/Harvard_University\n",
      "-----------------------------\n",
      "You succeeded in  116  steps\n",
      "You reached your input URL:  https://en.wikipedia.org/wiki/Facebook\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Facebook\"\n",
    "wiki_winner(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
